{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "*Reading material: https://medium.com/@dharamshikrupa/nlp-landscape-from-1960s-to-2020s-4055a89f7b47*\n",
    "\n",
    "NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.\n",
    "\n",
    "![](https://static.javatpoint.com/tutorial/nlp/images/what-is-nlp.png)\n",
    "\n",
    "### Real world applications?\n",
    "Contextual advertisements\n",
    "\n",
    "Email spam classification\n",
    "\n",
    "Removing unintended content, hate speech \n",
    "\n",
    "Election analysis through social media discussions\n",
    "\n",
    "Search engines\n",
    "\n",
    "Chatbots\n",
    "\n",
    "### Common NLP Tasks\n",
    "Text/Document classification\n",
    "\n",
    "Sentiment analysis\n",
    "\n",
    "Information retrieval from text/documents\n",
    "\n",
    "Parts of speech tagging\n",
    "\n",
    "Language detection and machine translation\n",
    "\n",
    "Conversational agents text and speech based\n",
    "\n",
    "Knowledge graphs (Neo4j)\n",
    "\n",
    "Text summarization\n",
    "\n",
    "Topic modelling (LTA)\n",
    "\n",
    "Text generation or word prediction\n",
    "\n",
    "Spell check and grammar corrections (Grammarly)\n",
    "\n",
    "Text parsing\n",
    "\n",
    "Speech to text\n",
    "\n",
    "### Approaches to NLP\n",
    "**Heuristic models**: Quick approach. Regular expressions, Wordnet (lexical dictionary), Open Mind Common Sense\n",
    "\n",
    "**ML Based models**: Rules for open ended problems are decided by machine based on data. Downside is that it the algos do not care about sequential information, feature generation has to be programmed e.g algorithms are Naives based, Logistics regression, SVM, LDA, Hidden Markov models. \n",
    "\n",
    "**DL based models**: Feature generation is automatic. DL Architectures for NLP RNN/LSTM/GRU/CNN/Transformers (BERT), Autoencoders\n",
    "\n",
    "### Challenges in NLP\n",
    "Spelling errors\n",
    "\n",
    "Synonyms\n",
    "\n",
    "Diversity of languages\n",
    "\n",
    "Ambiguity: I have never tasted a pizza quite like this before\n",
    "\n",
    "Contextual words: I ran to the store because we ran out of bread\n",
    "\n",
    "Colloquial/Slangs: Piece of cake, Pulling your leg\n",
    "\n",
    "Irony/Sarcasm/tone difference: yeah\n",
    "\n",
    "Creativity: Poems, scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to load dataset\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords # to get collection of stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "#import gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential   \n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "\n",
    "movie_reviews.head()\n",
    "\n",
    "#print(movie_reviews.shape)\n",
    "\n",
    "#print(movie_reviews.head())\n",
    "\n",
    "# Check for null values\n",
    "#movie_reviews.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.groupby(\"sentiment\").sentiment.count().plot.bar(ylim=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline is as follows\n",
    "\n",
    "*Reference: https://www.analyticsvidhya.com/blog/2022/06/an-end-to-end-guide-on-nlp-pipeline/*\n",
    "\n",
    "![Data Collection -> Text Preperation -> Feature Engineering -> Modelling -> Deployment\n",
    "](https://miro.medium.com/max/944/1*dWY7adQ62NDn_w_sc4lAKw.png)\n",
    "\n",
    "#### **a) Data Acquisition:**\n",
    "    \n",
    "#####    Available with me: \n",
    "        -Ready in CSV files\n",
    "        -in database. Pull data\n",
    "        -less data: Use data augmentation techniques without changing the sentiment. e.g. synonym replacement, bigram flip, back translate, add noise\n",
    "    \n",
    "#####    Available with others: \n",
    "        -Public dataset: Open public datasets \n",
    "        -Web scraping: beautifulsoup library (https://pypi.org/project/beautifulsoup4/) \n",
    "        -API: https://rapidapi.com/hub will help discover and connect thousands of APIs\n",
    "        -PDF/Image/Audio: Use python techniques\n",
    "\n",
    "#####    Unavailable:\n",
    "        -Create data with help of domain specialists\n",
    "        -Collect data through customer surveys\n",
    "\n",
    "#### **b) Text Preperation:**\n",
    "\n",
    "#####    Basic preprocessing and cleanup tasks: \n",
    "**Lowercasing** Avoid duplicating tokens\n",
    "\n",
    "**Remove HTML tags** Use regex to remove HTML tags use regex101.com to test\n",
    "\n",
    "**Remove URLs** \n",
    "```python\n",
    "            def remove_url (sentence) \n",
    "                pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "                return pattern.sub(r'', text)\n",
    "```\n",
    "**Removing punctuations, digits**: Punctuations can add to additional tokens and cause confusion. If punctuations `!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~` or digits does not contribute to the meaning you may remove them\n",
    "\n",
    "*`                Hey! How is it going?: 'Hey', '!', 'How', 'is', 'it', 'going', '?' or 'Hey!', 'How', 'is', 'it', 'going?'`*\n",
    "\n",
    "##### Assignment: Write an efficient python code to remove punctuation using translate().\n",
    "\n",
    "**Chat word treatment**: GM -> Good Morning, GN -> Good Night, AFK -> Away from keyboard etc. \n",
    "\n",
    "*Reference:[ ](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)*\n",
    "\n",
    "**Spelling checks**: \n",
    "```python\n",
    "            from textblob import TextBlob \n",
    "            textblb = TextBlob(incorrect_text) \n",
    "            textblb.correct()\n",
    "```\n",
    "\n",
    "**Stopword removal**: Words related to sentence formation e.g. 'and', 'or', 'the', 'an' etc. that does not contribute to the meaning. However, they will be retained for **POS tagging*. nklt and spaCy are 2 popular packages for removing stopwords\n",
    "\n",
    "*Reference: https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/*\n",
    "\n",
    "\n",
    "**Replace emojis**: Either remove or replace with Unicode normalization. \n",
    "\n",
    "*Reference: https://carpedm20.github.io/emoji/docs/index.html*\n",
    "\n",
    "**Tokenization**: **An important step** in NLP. Sentence or word tokenization. Prefix *$10*, Postfix *10km*, Infix *auto-encoder*, other exceptions is to split or to prevent splitting while removing punctuations *(U.S.A)* characters pose challenge while tokenizing \n",
    "\n",
    "*Reference: https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/*\n",
    "\n",
    "What are the challenges when using \n",
    "        a) split() function when applied to sentences like: Incredible India!, Where do you want to go today? etc.\n",
    "        b) RE is slighty better than split() function\n",
    "        c) NLTK packages applied to sentences like: Auto-rickshaw ride in Bangalore costs Rs.30.00/km, my email address is test@e.gov, Dr. Rajkumar received a Hon. Doctrate from Mysore\n",
    "        d) spaCy is a great for advance/complex NLP that gives better results. Try example sentences from above 3 options using spaCy\n",
    "\n",
    "\n",
    "**Stemming and Lemmatization**: End result is to get the root word \n",
    "\n",
    "*Inflection* a change in the form of a word (typically the ending) to express a grammatical function or attribute such as tense, mood, person, number, case, and gender. Tense inflection: walk-walking-walked, Plural inflection: dog-dogs\n",
    "\n",
    "*Stemming* is a process that reduces inflection from a word even if the stem word leads to incorrect meaning or spelling in the language. Stemming is used in case of information retrieval from large dataset faster. For instance, stemming the word ‘Caring‘ would return ‘Car‘. \n",
    "\n",
    "*Lemmatization* considers the context and converts the word to its meaningful base form, which is called **Lemma**. Lemmatization is computationally expensive since it involves look-up tables and what not. Used in chat applications. For instance, lemmatizing the word ‘Caring‘ would return ‘Care‘.\n",
    "\n",
    "*Reference: https://www.analyticsvidhya.com/blog/2022/06/stemming-vs-lemmatization-in-nlp-must-know-differences/*\n",
    "\n",
    "\n",
    "**Language detection**: For detecting language\n",
    "\n",
    "#####    Advanced Preprocessing tasks: (Park for now)\n",
    "**POS Tagging**: Each word will be assigned a part of speech\n",
    "**Parsing**: \n",
    "**Coreference resolution**:\n",
    "        \n",
    "![https://](https://shubhangidabral13.github.io/Bits-and-Bytes-of-NLP/images/copied_from_nb/my_icons/topic_02.b.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    \n",
    "    sen = re.sub('<.*?>', ' ', sen) # remove html tag\n",
    "\n",
    "    tokens = word_tokenize(sen)  # tokenizing words\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]    # lower case\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)  # remove punctuations\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    words = [word for word in stripped if word.isalpha()]  # remove non alphabet\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    words = [w for w in words if not w in stop_words]   # remove stop words\n",
    "    words = [w for w in words if len(w) > 2]  # Ignore words less than 2\n",
    "    \n",
    "    sentence = ' '.join(words)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sentiment from string to a binary form of 1 and 0, 1 is ‘positive’ and 0 is ‘negative’.\n",
    "y = movie_reviews['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the preprocessed reviews in a new list\n",
    "review_lines = []\n",
    "sentences = list(movie_reviews['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in sentences:\n",
    "    # preprocess each sentence of the review text\n",
    "    review_lines.append(preprocess_text(sen))\n",
    "\n",
    "print(len(review_lines))\n",
    "\n",
    "print(review_lines[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **c) Feature Extraction or Vectorization:**\n",
    "\n",
    "Converting text to vector representation is not a direct process like it is for Image or Speech data. Techniques used to convert text to vector representation should retain the semantic information of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Terms\n",
    "\n",
    "**Corpus** (C): Concatenation of all the strings\n",
    "\n",
    "**Vocabulary** (V): Unique words in the Corpus\n",
    "\n",
    "**Document** (D): Each sample is a document\n",
    "\n",
    "**Word** (W): Words of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techniques include\n",
    "\n",
    "**OHE**: One hot encoding is converting the words of your document into a V-dimension vector. The technique is intutive and easy to implement. This technique is not popular due to several flaws like sparsity results in overfitting, the vectors of the documents is usually varying in size, Out of Vocabulary problem, **does not capture semantic information** e.g Fruit, Orange, Shoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bag Of Words Model\n",
    "\n",
    "*Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BOW**: It is one of the most used text vectorization techniques. A bag-of-words is a representation of text that describes the occurrence of words within a document and order does not matter. Context does not matter either, but it gets captured. OOV does not occur. Performance is great in text classification. **Advantages**: This is simple and intutive, vector size is same across documents, in a way semantic information is captured. **Disadvantages**: Sparsity, OOV are ignored, order of words in sentences are not considered, if a small change in a sentence alter the meaning drastically BOW consideres both the sentences to be the same e.g. I like orange juice; I don't like orange juice\n",
    "\n",
    "*Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2500)\n",
    "\n",
    "X1 = cv.fit_transform(review_lines).toarray()\n",
    "\n",
    "print(cv.get_feature_names_out())\n",
    "\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pd.get_dummies(movie_reviews['sentiment'])\n",
    "y1 = y1.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train1.shape)\n",
    "print(y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model1 = MultinomialNB().fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(accuracy_score(y_test1, y_pred1))\n",
    "print(classification_report(y_pred1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = 'This is an average MoviE. I will not see it again'\n",
    "\n",
    "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "\n",
    "new_review = new_review.lower()\n",
    "\n",
    "new_review = new_review.split()\n",
    "\n",
    "new_review = ' '.join(new_review)\n",
    "\n",
    "new_X_test = cv.transform([new_review]).toarray()\n",
    "new_y_pred = model1.predict(new_X_test)\n",
    "print(new_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. n-grams\n",
    "\n",
    "*Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-n -grams model represents a text document as an unordered collection of its n-grams.\n",
    "\n",
    "**bi-gram** — using **two** words of the document\n",
    "\n",
    "**tri-gram** — using **three** words of the document\n",
    "\n",
    "**n-gram** — using **n** number of words of the document.\n",
    "\n",
    "**Advantages**: Able to capture the semantic meaning of the sentence\n",
    "\n",
    "**Disadvantages**: As we move from unigram to N-Gram then the dimension of vector formation increases and slows down the algorithm, no solution for OOV\n",
    "\n",
    "e.g food today was very good, food today was not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=2500, ngram_range=(2,2))\n",
    "\n",
    "X1 = cv.fit_transform(review_lines).toarray()\n",
    "\n",
    "print(cv.get_feature_names_out())\n",
    "\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tf-Idf\n",
    "\n",
    "*Reference: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. **Advantages**: \n",
    "\n",
    "*Term Frequency (TF)*: The number of times a word appears in a document is divided by the total number of words in that document. 0 < Tf< 1\n",
    "\n",
    "![\n",
    "](https://editor.analyticsvidhya.com/uploads/33409tf1.png)\n",
    "\n",
    "*Inverse Document Frequency (IDF)*: The logarithm of the number of documents in the corpus is divided by the number of documents where the specific term appears. In Scikit-learn use log(N/ni) + 1 formula.\n",
    "\n",
    "![\n",
    "](https://editor.analyticsvidhya.com/uploads/95671idf.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating tf-idf model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=2500)\n",
    "X2 = tv.fit_transform(review_lines).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = pd.get_dummies(movie_reviews['sentiment'])\n",
    "y2 = y2.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train2.shape)\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model2 = MultinomialNB().fit(X_train2, y_train2)\n",
    "\n",
    "y_pred2 = model2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test2,y_pred2))\n",
    "print(classification_report(y_pred2,y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = \"Please do not buy this product. The oats get cooked faster and the Quinoa does not cook at all. The end result is gooey porridge\"\n",
    "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "new_review = new_review.lower()\n",
    "new_review = new_review.split()\n",
    "new_review = ' '.join(new_review)\n",
    "print(new_review)\n",
    "new_X_test = cv.transform([new_review]).toarray()\n",
    "new_y_pred = model2.predict(new_X_test)\n",
    "print(new_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "In natural language processing (NLP), a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.[1] Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "\n",
    "There are **2** types: a) **Frequency based** (BOW, TF-Idf, Glove) b) **Prediction based** (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "**Word2Vec** is one of the most popular **Deep Learning** technique to learn word embeddings using shallow neural network. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
    "\n",
    "Core assumption is that two words sharing similar context also share similar meaning and consequently similar vector representation from the model.\n",
    "\n",
    "##### Advantages\n",
    "\n",
    "It can help extract semantic meaning (Happy, Joy are similar)\n",
    "\n",
    "Representation is based on dense vectors \n",
    "\n",
    "Dense vectors (mostly non-zero) are relative smaller helps in faster computations and prevents overfitting during training\n",
    "\n",
    "There are 2 types of algorithms namely: **CBOW - Continuous Bag Of Words** and **Skip-Gram** develop using shallow networks\n",
    "\n",
    "##### CBOW\n",
    "\n",
    "![\n",
    "](https://towardsmachinelearning.org/wp-content/uploads/2022/04/CBOW2.png)\n",
    "\n",
    "\n",
    "![\n",
    "](https://towardsmachinelearning.org/wp-content/uploads/2022/04/CBOW1.png)\n",
    "\n",
    "\n",
    "##### CBOW and Skip Gram Architectures\n",
    "\n",
    "![\n",
    "](https://www.researchgate.net/profile/Elena-Tutubalina/publication/318507923/figure/fig2/AS:613947946319904@1523388005889/Illustration-of-the-word2vec-models-a-CBOW-b-skip-gram-16-33.png)\n",
    "\n",
    "CBOW is used in case of small dataset whereas Skip-gram is used in large datasets\n",
    "\n",
    "*Reference: https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1*\n",
    "\n",
    "*Reference: https://jalammar.github.io/illustrated-word2vec/*\n",
    "\n",
    "*Word embeddings **visual inspector**: https://ronxin.github.io/wevi/*\n",
    "\n",
    "![\n",
    "](https://jalammar.github.io/images/word2vec/queen-woman-girl-embeddings.png)\n",
    "\n",
    "\n",
    "Word2Vec is available as a **pre-trained** model in addition to option of **training your own model**\n",
    "\n",
    "**Improve performance of Word2Vec**\n",
    "\n",
    "Increase training data set\n",
    "\n",
    "Increase vector dimension\n",
    "\n",
    "Increase window size\n",
    "\n",
    "Word2Vec pretrained model was trained on GoogleNews corupus containing 3B words. The model consists of 3 million words and phrases each represted as 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['man'].shape)\n",
    "\n",
    "print(model['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('ipl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('man', 'java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(['man', 'woman', 'java'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('horrible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![(https://](https://www.researchgate.net/publication/358432453/figure/fig1/AS:1121209337020446@1644328545976/Analogical-reasoning-on-vectors-a-king-man-womanqueen-and-b.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model['king'] - model['man'] + model['woman']\n",
    "model.most_similar([vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sent in review_lines:\n",
    "    sent_token = sent_tokenize(sent)\n",
    "    for sent in sent_token:\n",
    "        words.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model3 = gensim.models.Word2Vec(words, window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10378"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(doc):\n",
    "    return np.mean([model3.wv[word] for word in doc if word in model3.wv.index_to_key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10378/10378 [01:46<00:00, 97.10it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "X3 = []\n",
    "for i in tqdm(range(len(words))):\n",
    "    X3.append(avg_word2vec(words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10378, 100)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3=pd.get_dummies(movie_reviews['sentiment'])\n",
    "y3=y3.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_new,y3, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8302, 100)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8302,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model4 = SVC(kernel='rbf', random_state=0).fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = model4.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8063583815028902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.83      0.80       987\n",
      "        True       0.84      0.78      0.81      1089\n",
      "\n",
      "    accuracy                           0.81      2076\n",
      "   macro avg       0.81      0.81      0.81      2076\n",
      "weighted avg       0.81      0.81      0.81      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test3,y_pred3))\n",
    "print(classification_report(y_pred3,y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'dr', 'strange', 'mom', 'movie', 'was', 'not', 'great']\n",
      "['the dr strange mom movie was not great']\n",
      "[[-4.25027549e-01  3.76102239e-01 -1.17754415e-02  2.19848812e-01\n",
      "   9.62777995e-03 -9.79634583e-01  1.71377391e-01  1.49202728e+00\n",
      "  -1.26582170e+00 -3.83862674e-01 -2.68357009e-01 -8.35936248e-01\n",
      "  -5.61337888e-01  6.53094172e-01  3.23007882e-01 -6.55304134e-01\n",
      "   9.10625905e-02 -2.15287060e-01  2.48783261e-01 -7.99174070e-01\n",
      "   3.82544130e-01  3.41346234e-01  2.08509699e-01 -1.97810084e-01\n",
      "   8.06450844e-04  2.62502208e-03 -1.66353941e-01  4.00541760e-02\n",
      "  -7.20776796e-01 -4.74611342e-01 -1.08326040e-01 -4.10183191e-01\n",
      "   1.32701054e-01 -3.95946622e-01  5.54357842e-02  8.97217214e-01\n",
      "   5.82306981e-01 -6.55133665e-01 -4.89395976e-01 -6.37313575e-02\n",
      "   2.07061261e-01 -4.86977220e-01 -6.03871405e-01  1.23925321e-01\n",
      "   4.23423290e-01 -1.98031664e-01 -9.20679450e-01 -1.58006132e-01\n",
      "   6.66205287e-01  1.79108322e-01 -6.37286752e-02 -1.17133513e-01\n",
      "  -1.40488252e-01  1.04062617e-01 -8.18975449e-01  4.83119607e-01\n",
      "   4.91751790e-01 -1.76476866e-01  7.87247270e-02  2.37911016e-01\n",
      "   2.34105274e-01  3.38464379e-02 -1.42610684e-01 -4.63817567e-01\n",
      "  -7.42133856e-01  5.74559927e-01  5.36551923e-02  6.34060860e-01\n",
      "  -3.49942118e-01  9.23469782e-01 -2.62396753e-01  7.18587995e-01\n",
      "   4.79196697e-01 -1.38774991e-01  9.93701100e-01  3.27438325e-01\n",
      "   1.21699229e-01  2.06447095e-01 -4.61167425e-01  6.78630322e-02\n",
      "  -6.87804878e-01  3.13481390e-01 -4.86168385e-01  6.01754963e-01\n",
      "  -4.39556122e-01 -4.18005466e-01  2.69022495e-01  6.65602326e-01\n",
      "   6.53726578e-01  6.68898225e-02  1.00779438e+00  4.34124529e-01\n",
      "   3.77826989e-01  2.04627708e-01  1.01658869e+00  7.25262284e-01\n",
      "   7.85823464e-01 -4.15279031e-01  3.46463799e-01 -6.01146333e-02]]\n",
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "new_review = 'The Dr.Strange MOM movie was not great.'\n",
    "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "new_review = new_review.lower()\n",
    "new_review = new_review.split()\n",
    "\n",
    "print(new_review)\n",
    "#all_stopwords = stopwords.words('english')\n",
    "#all_stopwords.remove('not')\n",
    "#new_review = [lemmatizer.lemmatize(word) for word in new_review if not word in set(all_stopwords)]\n",
    "new_review = ' '.join(new_review)\n",
    "new_corpus = [new_review]\n",
    "print(new_corpus)\n",
    "\n",
    "new_words=[]\n",
    "for sent in new_corpus:\n",
    "    sent_token = sent_tokenize(sent)\n",
    "    for sent in sent_token:\n",
    "        new_words.append(simple_preprocess(sent))\n",
    "        \n",
    "new_X3 = []\n",
    "for i in range(len(new_words)):\n",
    "    new_X3.append(avg_word2vec(new_words[i]))\n",
    "\n",
    "print(new_X)    \n",
    "\n",
    "new_X = np.array(new_X3)\n",
    "new_y_pred = model4.predict(new_X)\n",
    "\n",
    "print(new_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
